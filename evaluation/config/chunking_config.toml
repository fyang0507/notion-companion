# Multi-lingual Chunking Configuration
# Supports Chinese, English, and French text

[chunking]
# Universal sentence punctuation
chinese_punctuation = ["。", "！", "？", "；", "......"]
western_punctuation = [".", "!", "?", "..."]

# Quotation mark pairs: [opening, closing]
# ASCII quotes use same character for open/close (ambiguous)
# Other quotes have distinct open/close characters (unambiguous)
quote_pairs = [
    ["\"", "\""],        # ASCII double quotes (ambiguous)
    ["'", "'"],          # ASCII single quotes (ambiguous)
    ["\u201c", "\u201d"],  # Curved/Chinese double quotes " " (unambiguous)
    ["\u2018", "\u2019"],  # Curved/Chinese single quotes ' ' (unambiguous)
    ["\u300c", "\u300d"],  # Chinese corner brackets 「 」 (unambiguous)
    ["\u300e", "\u300f"],  # Chinese double corner brackets 『 』 (unambiguous)
    ["\u00ab", "\u00bb"]   # French guillemets « » (unambiguous)
]

# English abbreviations that should NOT trigger sentence splits
english_abbreviations = [
    # Titles
    "Mr", "Mrs", "Ms", "Dr", "Prof", "Sr", "Jr",
    # Academic degrees
    "Ph\\.D", "B\\.A", "M\\.A", "B\\.S", "M\\.S", "B\\.Sc", "M\\.Sc",
    # Business
    "Inc", "Ltd", "Corp", "Co", "LLC",
    # Common abbreviations
    "etc", "vs", "i\\.e", "e\\.g", "cf", "ibid",
    # Time
    "a\\.m", "p\\.m", "AM", "PM",
    # Units
    "cm", "km", "kg", "lb", "oz", "ft", "in"
]

# French abbreviations
french_abbreviations = [
    # Titles
    "M", "Mme", "Mlle", "Dr", "Prof",
    # Common
    "c\\.-à-d", "p\\. ex", "cf", "etc"
]

[semantic_merging]
# Similarity threshold for merging adjacent sentences
similarity_threshold = 0.85

# Maximum number of sentences to merge in one chunk
max_merge_distance = 3

# Maximum chunk size in tokens (merging stops when this would be exceeded)
# 
# NOTE: Token counting depends on the tokenizer used:
# - OpenAI tokenizers (tiktoken): Subword-based tokenization
#   * English/French: Common words = 1 token, complex words = 2-4 tokens
#   * Chinese: Characters often 1-2 tokens each, but varies by context
#   * Example: "Hello world" = 2 tokens, "你好世界" = 4 tokens
# - Other tokenizers may count differently (whitespace, character-based, etc.)
# 
# For reference with OpenAI's cl100k_base tokenizer:
# - English: ~1 token per 0.75 words (750 words ≈ 1000 tokens)
# - Chinese: ~1 token per 0.5-1 characters (500-1000 characters ≈ 1000 tokens)
# - Mixed content: Token density varies by language ratio
max_chunk_size = 500

[embeddings]
# Embedding model for semantic similarity
model = "text-embedding-3-small"

# Batch size for embedding generation
batch_size = 32

# Enable caching of embeddings
enable_caching = true

[performance]
# Enable debug logging
debug_logging = false

# Enable performance metrics collection
enable_metrics = true

# Maximum text length to process in one operation
max_text_length = 100000 