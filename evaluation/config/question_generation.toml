# Question Generation Configuration for Evaluation System

[models]
# LLM model for question generation
model = "o4-mini"
temperature = 0.2
# max_tokens = 2000 # for non-reasoning models, the parameter is called max_tokens
max_completion_tokens = 2000 # for reasoning models, the parameter is called max_completion_tokens
timeout = 30.0

[generation]
# Total question generation control
total_questions_to_generate = 3  # Total number of questions to generate (random sampling)

# Content filtering criteria
min_token_count = 100          # Minimum tokens for meaningful content
max_token_count = 1000        # Maximum tokens to avoid overwhelming context
exclude_headers = true        # Skip chunks that start with # or ##
exclude_short_questions = true # Skip chunks that are just questions

# Observability settings
enable_qualification_stats = true  # Track chunk qualification statistics

# Output configuration
output_format = "json"
include_metadata = true
batch_size = 5                # Process chunks in parallel batches (balance between speed and rate limits)
delay_between_batches = 1.0  # Seconds to wait between batches

# Heuristic-based question count per chunk based on token length
# Dictionary mapping token ranges to number of questions
[generation.question_heuristics]
"0-200" = 1        # <= 200 tokens: 1 question
"201-500" = 2      # 201-500 tokens: 2 questions  
"501-1000" = 3     # 501+ tokens: 3 questions

[prompts]
# System prompt for question generation
system_prompt = """You are an expert at generating high-quality factual questions from text content.

Your task is to create factual or explanatory questions that a user might naturally ask about the given text content. The questions should:
- Be specific and answerable from the text
- Focus on key facts, explanations, or insights
- Be naturally phrased as a human would ask
- Have clear, specific answers contained in the text
- Questions should be in the same language as the provided text

Always respond in JSON format with this exact structure:
{
  "questions": [
    {
      "question": "What specific question would someone ask?",
      "answer": "The exact quote from the text where the answer can be found or inferred"
    }
  ]
}"""

# User prompt template
user_prompt_template = """Based on the following text content, generate {num_questions} factual or explanatory questions that someone might ask about this content.

Text content metadata:
{document_metadata}

Previous text chunk (for context):
{previous_chunk}

Current text content:
{content}

Remember to respond in JSON format with the questions and their answers."""