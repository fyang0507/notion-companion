# Model Configuration for Notion Companion
# Centralized configuration for all AI models used in the system

[models.embedding]
# Primary embedding model for documents and queries
model = "text-embedding-3-small"
dimensions = 1536
max_input_tokens = 8191
batch_size = 10  # Smaller batch for development

# Alternative embedding models (for future use)
# [models.embedding.alternatives]
# large = "text-embedding-3-large"  # Higher quality
# ada = "text-embedding-ada-002"    # Legacy model

[models.chat]
# Primary chat model for user interactions
model = "gpt-4o-mini"  # Cost-effective for development
max_tokens = 4096
temperature = 0.7

# Chat model alternatives (not currently used)
# [models.chat.alternatives]
# fast = "gpt-4o-mini"      # Faster for simple queries
# reasoning = "o1-preview"   # For complex reasoning tasks
# legacy = "gpt-4"          # Previous generation

[models.summarization]
# Model for document summarization
model = "gpt-4o-mini"  # Cost-effective for development
max_tokens = 800
temperature = 0.3  # Lower temperature for consistent summaries

# Summarization alternatives (not currently used)
# [models.summarization.alternatives]
# quality = "gpt-4o"        # Higher quality summaries
# budget = "gpt-3.5-turbo"  # Budget option

# Analysis model (not currently used)
# [models.analysis]
# model = "gpt-4o-mini"
# max_tokens = 1000
# temperature = 0.2

[limits]
# Token limits for different operations
max_embedding_tokens = 8000        # Conservative limit for embeddings
max_summary_input_tokens = 20000   # Max content to summarize at once
max_chat_context_tokens = 15000    # Max context for chat responses
chunk_size_tokens = 1000           # Default chunk size
chunk_overlap_tokens = 100         # Default chunk overlap

[performance]
# Performance and rate limiting settings
embedding_batch_size = 10          # Development-friendly batch size
embedding_delay_seconds = 0.1      # Delay between embedding calls
chat_delay_seconds = 0.5           # Delay between chat completions
summarization_delay_seconds = 1.0  # Delay between summarization calls
max_retries = 3                    # Max retries for failed API calls
retry_delay_seconds = 2.0          # Delay between retries

[vector_search]
# Vector similarity search parameters
match_threshold = 0.1              # Similarity threshold for relevant results (0.0-1.0)
match_count_default = 10           # Default number of results to return
match_count_max = 50               # Maximum number of results allowed

# Context enrichment settings
enable_context_enrichment = true   # Enable adjacent chunk context enrichment
adjacent_chunks_count = 2          # Number of adjacent chunks to retrieve (each side)
context_enrichment_timeout = 5.0   # Timeout for context enrichment in seconds

# Hybrid search settings
document_weight = 0.3              # Weight for document-level results in hybrid search
chunk_weight = 0.7                # Weight for chunk-level results in hybrid search

# Contextual embeddings settings (Anthropic-style contextual retrieval)
contextual_weight = 0.7            # Weight for contextual embeddings
content_weight = 0.3               # Weight for content embeddings
enable_contextual_fallback = true  # Fall back to content embeddings if contextual unavailable

# Re-ranking settings
enable_reranking = true            # Enable contextual re-ranking of results
context_boost_factor = 0.05        # Boost for results with context information
summary_boost_factor = 0.03        # Boost for results with summaries
section_boost_factor = 0.02        # Boost for results with section information

[chat_interface]
# Chat interface display and processing settings
top_sources_limit = 5              # Maximum number of sources to include in context
context_content_length = 500       # Characters of content to include in context per source
citation_preview_length = 200      # Characters to show in citation previews
message_preview_length = 100       # Characters to show in log message previews
max_context_sources = 10           # Maximum sources to consider before filtering to top sources

# ============================================================================
# PROMPT TEMPLATES - Centralized prompt management for active LLM processes
# ============================================================================

[prompts.chat]
# Main chat assistant system prompt
system_prompt = """You are a helpful AI assistant that answers questions based on the user's Notion workspace content. 
{context_section}

Guidelines:
- Respond in the SAME LANGUAGE as the user's question
- Be concise and helpful
- Reference specific documents when possible
- If you're not sure about something, say so
- Format responses in markdown when appropriate
- Maintain the user's preferred language throughout the conversation"""

# Streaming chat system prompt (shorter for efficiency)
streaming_system_prompt = """You are a helpful AI assistant that answers questions based on the user's Notion workspace content. 
{context_section}

Respond in the SAME LANGUAGE as the user's question. Be concise and helpful."""

# Context section template (inserted into system prompts)
context_template = "Here is relevant context from their workspace: {context}"

[prompts.title_generation]
# Chat title generation prompt
title_prompt = """Based on this conversation, generate a concise, descriptive title that captures the main topic or question being discussed. 

Guidelines:
- Maximum {max_words} words
- Be specific and descriptive
- Focus on the main topic/question
- Use clear, simple language
- No quotes or special formatting
- No articles (a, an, the) unless essential

Conversation:
{conversation_text}

Title ({max_words} words max):"""

# Title generation settings
max_words = 8
temperature_override = 0.3  # Lower temperature for consistency
max_tokens_override = 20    # Short response for titles

[prompts.summarization]
# Document summarization prompt
document_summary_prompt = """Please create a comprehensive but concise summary of this document that captures:
1. Main topics and key points
2. Important concepts and themes  
3. Essential information and takeaways
4. Context and purpose

The summary should be roughly {max_length} words and be optimized for semantic search.

Title: {title}

Content:
{content}

Summary:"""

# Chat conversation summarization prompt
chat_summary_prompt = """Generate a concise 1-2 sentence summary of this conversation. Focus on the main topic and key points discussed.

Conversation:
{conversation_text}

Summary (max 150 characters):"""

# Summarization settings
default_max_length = 500   # Default max words for document summaries
chat_summary_max_tokens = 40
chat_summary_temperature = 0.3
chat_summary_max_chars = 150

# ============================================================================
# FUTURE PROMPTS - Ready for implementation but commented out for lean config
# ============================================================================

# [prompts.document_processing]
# Title: {title}
# Content: {content}
# 
# Classify as one of: documentation, meeting_notes, project_plan, knowledge_base, reference, other
# 
# Content Type:"""
# 
# # Metadata extraction prompt (for future use)
# metadata_extraction_prompt = """Extract key metadata from this document that would be useful for search and organization.
# 
# Title: {title}
# Content: {content}
# 
# Extract:
# - Key topics (comma-separated)
# - Main categories (comma-separated)
# - Priority level (high/medium/low)
# - Document type (technical/business/personal/other)
# 
# Metadata:"""

# [prompts.search]
# # Query enhancement prompts (for future use)
# query_expansion_prompt = """Expand this search query to improve semantic search results. Consider synonyms, related terms, and different phrasings.
# 
# Original query: {query}
# 
# Expanded query suggestions (comma-separated):"""
# 
# # Search result reranking prompt (for future use)
# result_ranking_prompt = """Given this search query and these results, rank them by relevance and provide a brief explanation.
# 
# Query: {query}
# 
# Results:
# {results}
# 
# Ranked results with explanations:"""

# Optimization settings (not currently used)
# [optimization]
# enable_caching = true               # Cache embeddings and summaries
# cache_ttl_days = 30                # Cache time-to-live
# prefer_faster_models = false       # Use faster alternatives when possible