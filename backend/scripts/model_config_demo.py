#!/usr/bin/env python3
"""
LLM Configuration System Demo

Comprehensive demonstration of the centralized LLM configuration system that manages
all AI model configurations and prompts for the Notion Companion webapp.
"""

import sys
from pathlib import Path

# Add backend to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from config.model_config import ModelConfigManager

def main():
    print("ğŸ”§ LLM Configuration System Demo")
    print("=" * 50)
    
    # Initialize configuration manager
    print("\n1. ğŸš€ Initializing Configuration Manager")
    config = ModelConfigManager()
    print("âœ… Configuration loaded successfully from models.toml")
    
    # Display model configurations
    print("\n2. ğŸ¤– Model Configurations")
    print("-" * 30)
    
    embedding = config.get_embedding_config()
    print(f"ğŸ”¤ Embedding: {embedding.model}")
    print(f"   â€¢ Dimensions: {embedding.dimensions}")
    print(f"   â€¢ Max Input Tokens: {embedding.max_input_tokens:,}")
    print(f"   â€¢ Batch Size: {embedding.batch_size}")
    
    chat = config.get_chat_config()
    print(f"ğŸ’¬ Chat: {chat.model}")
    print(f"   â€¢ Max Tokens: {chat.max_tokens:,}")
    print(f"   â€¢ Temperature: {chat.temperature}")
    
    summary = config.get_summarization_config()
    print(f"ğŸ“ Summarization: {summary.model}")
    print(f"   â€¢ Max Tokens: {summary.max_tokens}")
    print(f"   â€¢ Temperature: {summary.temperature}")
    
    # Display processing limits
    print("\n3. ğŸ“Š Processing Limits")
    print("-" * 30)
    limits = config.get_limits_config()
    print(f"Max Embedding Tokens: {limits.max_embedding_tokens:,}")
    print(f"Max Summary Input: {limits.max_summary_input_tokens:,}")
    print(f"Max Chat Context: {limits.max_chat_context_tokens:,}")
    print(f"Chunk Size: {limits.chunk_size_tokens:,}")
    print(f"Chunk Overlap: {limits.chunk_overlap_tokens}")
    
    # Display performance settings
    print("\n4. âš¡ Performance Settings")
    print("-" * 30)
    perf = config.get_performance_config()
    print(f"Embedding Batch Size: {perf.embedding_batch_size}")
    print(f"Embedding Delay: {perf.embedding_delay_seconds}s")
    print(f"Chat Delay: {perf.chat_delay_seconds}s")
    print(f"Max Retries: {perf.max_retries}")
    print(f"Retry Delay: {perf.retry_delay_seconds}s")
    
    # Show prompt management capabilities
    print("\n5. ğŸ“ Prompt Management")
    print("-" * 30)
    prompts = config.get_prompts_config()
    print(f"Title Max Words: {prompts.title_generation.max_words}")
    print(f"Title Temperature Override: {prompts.title_generation.temperature_override}")
    print(f"Title Max Tokens Override: {prompts.title_generation.max_tokens_override}")
    print(f"Default Summary Length: {prompts.summarization.default_max_length} words")
    print(f"Chat Summary Max Tokens: {prompts.summarization.chat_summary_max_tokens}")
    print(f"Chat Summary Max Characters: {prompts.summarization.chat_summary_max_chars}")
    
    # Demonstrate dynamic prompt formatting
    print("\n6. ğŸ¨ Dynamic Prompt Formatting")
    print("-" * 30)
    
    # Sample data for demonstrations
    sample_context = "Document 1: Q4 Planning Meeting Notes\nDocument 2: Product Roadmap 2024"
    sample_conversation = "User: What are our Q4 goals?\nAssistant: Based on your planning documents, the main Q4 goals include increasing user engagement by 25% and launching 3 new features."
    
    # Chat system prompt with context
    chat_prompt = config.format_chat_system_prompt(context=sample_context)
    print("ğŸ’¬ Chat System Prompt (with context):")
    print(f"   Length: {len(chat_prompt)} characters")
    print(f"   Preview: '{chat_prompt[:80]}...'")
    
    # Streaming chat prompt
    streaming_prompt = config.format_chat_system_prompt(context=sample_context, use_streaming=True)
    print("ğŸ”„ Streaming Chat Prompt:")
    print(f"   Length: {len(streaming_prompt)} characters")
    print(f"   Preview: '{streaming_prompt[:80]}...'")
    
    # Title generation prompt
    title_prompt = config.format_title_prompt(sample_conversation)
    print("ğŸ·ï¸  Title Generation Prompt:")
    print(f"   Length: {len(title_prompt)} characters")
    print(f"   Preview: '{title_prompt[:80]}...'")
    
    # Document summary prompt
    doc_summary_prompt = config.format_document_summary_prompt(
        title="Q4 Planning Meeting", 
        content="We discussed quarterly goals, key metrics, timeline, and resource allocation for Q4 2024..."
    )
    print("ğŸ“„ Document Summary Prompt:")
    print(f"   Length: {len(doc_summary_prompt)} characters")
    print(f"   Preview: '{doc_summary_prompt[:80]}...'")
    
    # Chat summary prompt
    chat_summary_prompt = config.format_chat_summary_prompt(sample_conversation)
    print("ğŸ’­ Chat Summary Prompt:")
    print(f"   Length: {len(chat_summary_prompt)} characters")
    print(f"   Preview: '{chat_summary_prompt[:80]}...'")
    
    # Demonstrate service integration simulation
    print("\n7. ğŸ”§ Service Integration Simulation")
    print("-" * 30)
    print("Simulating how services would use centralized configuration:")
    print(f"âœ… Chat responses use: {chat.model} (temp: {chat.temperature})")
    print(f"âœ… Title generation uses: {chat.model} (temp: {prompts.title_generation.temperature_override})")
    print(f"âœ… Document summaries use: {summary.model} (temp: {summary.temperature})")
    print(f"âœ… Embeddings use: {embedding.model} ({embedding.dimensions}D, batch: {embedding.batch_size})")
    print("âœ… All prompts are centrally managed and dynamically formatted")
    
    # Show LLM-enabled processes
    print("\n8. ğŸ¯ LLM-Enabled Processes")
    print("-" * 30)
    
    active_processes = [
        ("ğŸ’¬ Chat Responses", f"{chat.model} â€¢ RAG-enhanced conversations"),
        ("ğŸ·ï¸ Title Generation", f"{chat.model} â€¢ {prompts.title_generation.max_words}-word session titles"),
        ("ğŸ“ Chat Summaries", f"{summary.model} â€¢ {prompts.summarization.chat_summary_max_chars}-char summaries"),
        ("ğŸ“„ Document Summaries", f"{summary.model} â€¢ {prompts.summarization.default_max_length}-word summaries"),
        ("ğŸ“Š Embeddings", f"{embedding.model} â€¢ {embedding.dimensions}D vectors"),
        ("ğŸ” Document Processing", "Content extraction and chunking"),
    ]
    
    print("Currently Active:")
    for process, details in active_processes:
        print(f"  {process}: {details}")
    
    future_processes = [
        "ğŸ” Query Expansion (prompts ready)",
        "ğŸ“Š Result Ranking (prompts ready)", 
        "ğŸ·ï¸ Content Classification (prompts ready)",
        "ğŸ“Š Metadata Extraction (prompts ready)"
    ]
    
    print("\nFuture-Ready:")
    for process in future_processes:
        print(f"  {process}")
    
    # Configuration reloading test
    print("\n9. ğŸ”„ Configuration Management")
    print("-" * 30)
    print("Testing configuration reload capability...")
    config.reload_config()
    print("âœ… Configuration reloaded successfully")
    print("ğŸ’¡ For production: edit models.toml values and redeploy")
    
    # Summary and key benefits
    print("\n" + "=" * 50)
    print("âœ… LLM Configuration System Demo Complete!")
    print("\nğŸŒŸ Key Benefits:")
    print("â€¢ ğŸ¯ Single source of truth for all AI models and prompts")
    print("â€¢ ğŸ”§ Type-safe configuration with validation")
    print("â€¢ ğŸ¨ Dynamic prompt formatting with context injection")
    print("â€¢ ğŸš€ Easy production deployment (update models.toml)")
    print("â€¢ ğŸ“ˆ Future-proof architecture for new LLM features")
    print("â€¢ ğŸ’° Cost-optimized defaults for development")
    
    print("\nğŸ”§ Usage:")
    print("â€¢ Development: Use current settings (gpt-4o-mini, small batches)")
    print("â€¢ Production: Update models.toml with gpt-4o, larger batches")
    print("â€¢ Customization: Modify prompts in models.toml as needed")

if __name__ == "__main__":
    main()